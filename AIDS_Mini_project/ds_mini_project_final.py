# -*- coding: utf-8 -*-
"""ds_mini_project_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JkcXvTVfDOZuWWSbIezXGxo3YEiSUQHo
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import numpy as np

file_path = "/content/dataset_with_price_category_corrected.csv"  # Upload this file in Colab
df = pd.read_csv(file_path)
print(df.head())

print(df.describe())
print(df.isnull().sum())

plt.figure(figsize=(6, 4))
sns.countplot(x=df["Price Category"])
plt.title("Price Category Distribution")
plt.show()

if "Date" in df.columns:
    df["Date"] = pd.to_datetime(df["Date"], errors="coerce")
    df = df.drop(columns=["Date"])

numeric_df = df.select_dtypes(include=["number"])

plt.figure(figsize=(12, 6))
sns.heatmap(numeric_df.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Feature Correlation Heatmap")
plt.show()

plt.figure(figsize=(12, 6))
sns.boxplot(x="Crop Type", y="Price (₹/kg)", data=df)
plt.xticks(rotation=90)
plt.title("Price Distribution Across Crops")
plt.show()

categorical_cols = ["State", "City", "Crop Type", "Season", "Price Category"]
label_encoders = {}

for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

X = df.drop(columns=["Price Category", "Price (₹/kg)"])
y_classification = df["Price Category"]
y_regression = df["Price (₹/kg)"]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print(pd.DataFrame(X_scaled, columns=X.columns).head())

plt.figure(figsize=(10, 5))
sns.boxplot(data=pd.DataFrame(X_scaled, columns=X.columns))
plt.xticks(rotation=90)
plt.title("Scaled Feature Distributions")
plt.show()

X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_scaled, y_classification, test_size=0.2, random_state=42)

X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_scaled, y_regression, test_size=0.2, random_state=42)

dtc = DecisionTreeClassifier(random_state=42)
dtc.fit(X_train_c, y_train_c)
y_pred_dtc = dtc.predict(X_test_c)

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_c, y_train_c)
y_pred_knn = knn.predict(X_test_c)

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

def evaluate_classification(y_test, y_pred, model_name):
    print(f"\n{model_name} Classification Metrics:")

    # Automatically select the correct average based on class count
    average_method = 'binary' if len(np.unique(y_test)) == 2 else 'weighted'

    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
    print(f"Precision: {precision_score(y_test, y_pred, average=average_method):.4f}")
    print(f"Recall: {recall_score(y_test, y_pred, average=average_method):.4f}")
    print(f"F1 Score: {f1_score(y_test, y_pred, average=average_method):.4f}")

    # Generate confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    print("\nConfusion Matrix (Values):")
    print(cm)

    # Ensure proper axis labels based on class labels
    class_labels = np.unique(y_test)

    # Plot confusion matrix
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="coolwarm",
                xticklabels=class_labels, yticklabels=class_labels)
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title(f"{model_name} Confusion Matrix")
    plt.show()

evaluate_classification(y_test_c, y_pred_dtc, "Decision Tree")

evaluate_classification(y_test_c, y_pred_knn, "KNN")

# Hyperparameter tuning for Decision Tree
param_grid_dtc = {
    "max_depth": [5, 10, 15, None],
    "min_samples_split": [2, 5, 10]
}

grid_dtc = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_dtc, cv=5, scoring="accuracy", n_jobs=-1)
grid_dtc.fit(X_train_c, y_train_c)

print("Best parameters for Decision Tree:", grid_dtc.best_params_)

# Evaluate tuned model
best_dtc = grid_dtc.best_estimator_
y_pred_tuned_dtc = best_dtc.predict(X_test_c)
evaluate_classification(y_test_c, y_pred_tuned_dtc, "Tuned Decision Tree")

knn_reg = KNeighborsRegressor(n_neighbors=5)
knn_reg.fit(X_train_r, y_train_r)
y_pred_knn_reg = knn_reg.predict(X_test_r)

# Train Decision Tree Regressor
dtr = DecisionTreeRegressor(random_state=42)
dtr.fit(X_train_r, y_train_r)
y_pred_dtr = dtr.predict(X_test_r)

def evaluate_regression(y_test, y_pred, model_name):
    print(f"\n{model_name} Regression Metrics:")
    print(f"R2 Score: {r2_score(y_test, y_pred):.4f}")
    print(f"MSE: {mean_squared_error(y_test, y_pred):.4f}")
    print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}")
    print(f"MAE: {mean_absolute_error(y_test, y_pred):.4f}")

evaluate_regression(y_test_r, y_pred_knn_reg, "KNN Regressor")

plt.figure(figsize=(8, 6))
plt.scatter(y_test_r, y_pred_dtr, color="blue", alpha=0.5, label="Predicted vs Actual")
plt.plot(y_test_r, y_test_r, color="red", linestyle="dashed", label="Perfect Fit (y = x)")
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs Predicted Prices")
plt.legend()
plt.show()

evaluate_regression(y_test_r, y_pred_dtr, "Decision Tree Regressor")

plt.figure(figsize=(8, 6))
plt.scatter(y_test_r, y_pred_knn_reg, color="green", alpha=0.5, label="Predicted vs Actual")
plt.plot(y_test_r, y_test_r, color="red", linestyle="dashed", label="Perfect Fit (y = x)")
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs Predicted Prices (Non-Tuned KNN Regressor)")
plt.legend()
plt.show()

import joblib

joblib.dump(best_dtc, "decision_tree_classifier.pkl")
joblib.dump(best_dtr, "decision_tree_regressor.pkl")

from google.colab import files

files.download("decision_tree_classifier.pkl")
files.download("decision_tree_regressor.pkl")

